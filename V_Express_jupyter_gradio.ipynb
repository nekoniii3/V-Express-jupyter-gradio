{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "L4",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/nekoniii3/V-Express-jupyter-gradio/blob/main/V_Express_jupyter_gradio.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **V_Express-Jupyter-Gradio**\n",
        "This is a modification of https://colab.research.google.com/github/camenduru/V-Express-jupyter/"
      ],
      "metadata": {
        "id": "k2HX4TkVj3C1"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VjYy0F2gZIPR"
      },
      "outputs": [],
      "source": [
        "%cd /content\n",
        "!git clone -b dev https://github.com/camenduru/V-Express\n",
        "%cd /content/V-Express\n",
        "\n",
        "!apt -y install -qq aria2\n",
        "\n",
        "!aria2c --console-log-level=error -c -x 16 -s 16 -k 1M https://huggingface.co/camenduru/V-Express/resolve/main/insightface_models/models/buffalo_l/1k3d68.onnx -d /content/V-Express/model_ckpts/insightface_models/models/buffalo_l -o 1k3d68.onnx\n",
        "!aria2c --console-log-level=error -c -x 16 -s 16 -k 1M https://huggingface.co/camenduru/V-Express/resolve/main/insightface_models/models/buffalo_l/2d106det.onnx -d /content/V-Express/model_ckpts/insightface_models/models/buffalo_l -o 2d106det.onnx\n",
        "!aria2c --console-log-level=error -c -x 16 -s 16 -k 1M https://huggingface.co/camenduru/V-Express/resolve/main/insightface_models/models/buffalo_l/det_10g.onnx -d /content/V-Express/model_ckpts/insightface_models/models/buffalo_l -o det_10g.onnx\n",
        "!aria2c --console-log-level=error -c -x 16 -s 16 -k 1M https://huggingface.co/camenduru/V-Express/resolve/main/insightface_models/models/buffalo_l/genderage.onnx -d /content/V-Express/model_ckpts/insightface_models/models/buffalo_l -o genderage.onnx\n",
        "!aria2c --console-log-level=error -c -x 16 -s 16 -k 1M https://huggingface.co/camenduru/V-Express/resolve/main/insightface_models/models/buffalo_l/w600k_r50.onnx -d /content/V-Express/model_ckpts/insightface_models/models/buffalo_l -o w600k_r50.onnx\n",
        "\n",
        "!aria2c --console-log-level=error -c -x 16 -s 16 -k 1M https://huggingface.co/camenduru/V-Express/raw/main/sd-vae-ft-mse/config.json -d /content/V-Express/model_ckpts/sd-vae-ft-mse -o config.json\n",
        "!aria2c --console-log-level=error -c -x 16 -s 16 -k 1M https://huggingface.co/camenduru/V-Express/resolve/main/sd-vae-ft-mse/diffusion_pytorch_model.bin -d /content/V-Express/model_ckpts/sd-vae-ft-mse -o diffusion_pytorch_model.bin\n",
        "\n",
        "!aria2c --console-log-level=error -c -x 16 -s 16 -k 1M https://huggingface.co/camenduru/V-Express/raw/main/stable-diffusion-v1-5/unet/config.json -d /content/V-Express/model_ckpts/stable-diffusion-v1-5/unet -o config.json\n",
        "\n",
        "!aria2c --console-log-level=error -c -x 16 -s 16 -k 1M https://huggingface.co/camenduru/V-Express/resolve/main/v-express/audio_projection.pth -d /content/V-Express/model_ckpts/v-express -o audio_projection.pth\n",
        "!aria2c --console-log-level=error -c -x 16 -s 16 -k 1M https://huggingface.co/camenduru/V-Express/resolve/main/v-express/denoising_unet.pth -d /content/V-Express/model_ckpts/v-express -o denoising_unet.pth\n",
        "!aria2c --console-log-level=error -c -x 16 -s 16 -k 1M https://huggingface.co/camenduru/V-Express/resolve/main/v-express/motion_module.pth -d /content/V-Express/model_ckpts/v-express -o motion_module.pth\n",
        "!aria2c --console-log-level=error -c -x 16 -s 16 -k 1M https://huggingface.co/camenduru/V-Express/resolve/main/v-express/reference_net.pth -d /content/V-Express/model_ckpts/v-express -o reference_net.pth\n",
        "!aria2c --console-log-level=error -c -x 16 -s 16 -k 1M https://huggingface.co/camenduru/V-Express/resolve/main/v-express/v_kps_guider.pth -d /content/V-Express/model_ckpts/v-express -o v_kps_guider.pth\n",
        "\n",
        "!aria2c --console-log-level=error -c -x 16 -s 16 -k 1M https://huggingface.co/camenduru/V-Express/raw/main/wav2vec2-base-960h/config.json -d /content/V-Express/model_ckpts/wav2vec2-base-960h -o config.json\n",
        "!aria2c --console-log-level=error -c -x 16 -s 16 -k 1M https://huggingface.co/camenduru/V-Express/raw/main/wav2vec2-base-960h/feature_extractor_config.json -d /content/V-Express/model_ckpts/wav2vec2-base-960h -o feature_extractor_config.json\n",
        "!aria2c --console-log-level=error -c -x 16 -s 16 -k 1M https://huggingface.co/camenduru/V-Express/raw/main/wav2vec2-base-960h/preprocessor_config.json -d /content/V-Express/model_ckpts/wav2vec2-base-960h -o preprocessor_config.json\n",
        "!aria2c --console-log-level=error -c -x 16 -s 16 -k 1M https://huggingface.co/camenduru/V-Express/resolve/main/wav2vec2-base-960h/pytorch_model.bin -d /content/V-Express/model_ckpts/wav2vec2-base-960h -o pytorch_model.bin\n",
        "!aria2c --console-log-level=error -c -x 16 -s 16 -k 1M https://huggingface.co/camenduru/V-Express/raw/main/wav2vec2-base-960h/special_tokens_map.json -d /content/V-Express/model_ckpts/wav2vec2-base-960h -o special_tokens_map.json\n",
        "!aria2c --console-log-level=error -c -x 16 -s 16 -k 1M https://huggingface.co/camenduru/V-Express/raw/main/wav2vec2-base-960h/tokenizer_config.json -d /content/V-Express/model_ckpts/wav2vec2-base-960h -o tokenizer_config.json\n",
        "!aria2c --console-log-level=error -c -x 16 -s 16 -k 1M https://huggingface.co/camenduru/V-Express/raw/main/wav2vec2-base-960h/vocab.json -d /content/V-Express/model_ckpts/wav2vec2-base-960h -o vocab.json\n",
        "\n",
        "# Gradio追加\n",
        "!pip install gradio==4.32.1\n",
        "\n",
        "!pip install diffusers==0.24.0 imageio-ffmpeg==0.4.9 insightface==0.7.3 omegaconf==2.2.3 onnxruntime==1.16.3 safetensors==0.4.2 transformers==4.30.2 einops==0.4.1 tqdm==4.66.1 xformers==0.0.26.post1 av accelerate\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **①画像と音声のみで動画を生成（シナリオ2）**\n",
        "Generate videos using only images and audio（Scenario2）"
      ],
      "metadata": {
        "id": "fWE5P16bjF3E"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WGORu9Pv5cO5"
      },
      "outputs": [],
      "source": [
        "%cd /content/V-Express\n",
        "\n",
        "import gradio as gr\n",
        "import os\n",
        "import datetime\n",
        "from zoneinfo import ZoneInfo\n",
        "\n",
        "dt = datetime.datetime.now(ZoneInfo(\"Asia/Tokyo\"))\n",
        "fol_name = dt.strftime(\"%Y%m%d%H%M%S\")\n",
        "\n",
        "def fix_face_video(input_image, input_audio):\n",
        "\n",
        "    retarget_strategy = \"fix_face\"\n",
        "    num_inference_steps = 25\n",
        "\n",
        "    dt = datetime.datetime.now(ZoneInfo(\"Asia/Tokyo\"))\n",
        "    fol_name = dt.strftime(\"%Y%m%d\")\n",
        "    file_name = dt.strftime(\"%H%M%S\")\n",
        "\n",
        "    out_video = \"./output/\" + fol_name+ \"/fix_face_\" + file_name + \".mp4\"\n",
        "\n",
        "    !python inference.py \\\n",
        "        --reference_image_path $input_image \\\n",
        "        --audio_path $input_audio \\\n",
        "        --output_path $out_video \\\n",
        "        --retarget_strategy $retarget_strategy \\\n",
        "        --num_inference_steps $num_inference_steps\n",
        "\n",
        "    return out_video\n",
        "\n",
        "image = gr.Image(label=\"対象画像(jpg, png)\", type=\"filepath\")\n",
        "audio = gr.File(label=\"音声ファイル(mp3)\", file_types=[\".mp3\", \".MP3\"])\n",
        "out_video = gr.Video(label=\"Fix Face Video\")\n",
        "btn = gr.Button(\"送信\", variant=\"primary\")\n",
        "\n",
        "title = \"V_Express Scenario2\"\n",
        "description = \"<div style='text-align: center;'><h3>これは公式のシナリオ2（画像と話す音声による生成）のみ対応しています。\"\n",
        "description += \"<br>This only supports official Scenario 2 (Generated from images and speaking voices).</h3></div>\"\n",
        "\n",
        "demo = gr.Interface(\n",
        "    fn=fix_face_video,\n",
        "    inputs=[image, audio],\n",
        "    outputs=[out_video],\n",
        "    title=title,\n",
        "    submit_btn=btn,\n",
        "    clear_btn=None,\n",
        "    description=description,\n",
        "    allow_flagging=\"never\"\n",
        ")\n",
        "\n",
        "demo.queue()\n",
        "demo.launch(share=True, debug=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **②画像・音声・顔の情報で動画を生成（シナリオ1,3）**\n",
        "Generate videos using images, voice and face sequences（Scenario1,3）"
      ],
      "metadata": {
        "id": "lNSk444n0KAe"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gcAKL4jD0HNr"
      },
      "outputs": [],
      "source": [
        "%cd /content/V-Express\n",
        "\n",
        "import gradio as gr\n",
        "import os\n",
        "import datetime\n",
        "from zoneinfo import ZoneInfo\n",
        "\n",
        "def ex_kps_audio(input_video):\n",
        "\n",
        "    fol_name = \"kps_and_audio\"\n",
        "\n",
        "    # フォルダ作成\n",
        "    os.makedirs(fol_name, exist_ok=True)\n",
        "\n",
        "    input_video_name = os.path.splitext(os.path.basename(input_video))[0]\n",
        "\n",
        "    out_kps = \"./\" + fol_name+ \"/\" + input_video_name + \"_kps.pth\"\n",
        "    print(out_kps)\n",
        "\n",
        "    out_aud = \"./\" + fol_name+ \"/\" + input_video_name + \"_aud.mp3\"\n",
        "    print(out_aud)\n",
        "\n",
        "    !python scripts/extract_kps_sequence_and_audio.py \\\n",
        "        --video_path $input_video \\\n",
        "        --kps_sequence_save_path $out_kps \\\n",
        "        --audio_save_path $out_aud\n",
        "\n",
        "    return [out_kps, out_aud], out_kps, out_aud\n",
        "\n",
        "def fix_face_video(retarget_strategy, image, input_kps, input_audio):\n",
        "\n",
        "    num_inference_steps = 25\n",
        "\n",
        "    dt = datetime.datetime.now(ZoneInfo(\"Asia/Tokyo\"))\n",
        "    fol_name = dt.strftime(\"%Y%m%d\")\n",
        "    file_name = dt.strftime(\"%H%M%S\")\n",
        "\n",
        "    out_video = \"./output/\" + fol_name+ \"/fix_face_\" + file_name + \".mp4\"\n",
        "    print(out_video)\n",
        "\n",
        "    !python inference.py \\\n",
        "        --reference_image_path $image \\\n",
        "        --audio_path $input_audio \\\n",
        "        --kps_path $input_kps \\\n",
        "        --output_path $out_video \\\n",
        "        --retarget_strategy $retarget_strategy \\\n",
        "        --num_inference_steps $num_inference_steps\n",
        "\n",
        "    return out_video\n",
        "\n",
        "with gr.Blocks() as demo:\n",
        "\n",
        "    # make audio and face V-kps\n",
        "    title = \"<div style='text-align: center;'><h2>V_Express Scenario1,3</h2></div>\"\n",
        "    description = \"<div style='text-align: center;'><h3>これは公式のシナリオ1,3（動画と画像から生成）に対応しています。最初に動画をアップロードして下さい。\"\n",
        "    description += \"<br>This supports official Scenario1,3 (Generated from videos and images).Upload the video first.</h3></div>\"\n",
        "\n",
        "    gr.Markdown(title + description)\n",
        "\n",
        "    with gr.Row():\n",
        "        input_video = gr.Video(label=\"Input Video\")\n",
        "        kps_audio_file = gr.Files(label=\"V-kps and Audio\")\n",
        "\n",
        "    btn_vkps = gr.Button(\"抽出\")\n",
        "\n",
        "    gr.Markdown(\"<br><br>\")\n",
        "\n",
        "    # make fix face video\n",
        "    description = \"<div style='text-align: center;'><h3>画像をアップロードして、モードを選んでから実行して下さい。（シナリオ3が\\\"offset_retarget\\\"、シナリオ1が\\\"no_retarget\\\"）\"\n",
        "    description += \"<br>Upload image and select the mode, and run it.（Scenario3 \\\"offset_retarget\\\"、Scenario1 が\\\"no_retarget\\\"）</h3></div>\"\n",
        "\n",
        "    gr.Markdown(description)\n",
        "\n",
        "    with gr.Row():\n",
        "        image = gr.Image(label=\"対象画像(jpg, png)\", type=\"filepath\")\n",
        "        input_kps = gr.File(label=\"kps\")\n",
        "\n",
        "    with gr.Row():\n",
        "        input_audio = gr.File(file_types=[\".mp3\"], label=\"音声ファイル\")\n",
        "        out_video = gr.Video(label=\"Fix Face Video\")\n",
        "\n",
        "    retarget_strategy = gr.Dropdown(label=\"retarget_strategy\", choices=[\"offset_retarget\", \"no_retarget\"], value = \"offset_retarget\")\n",
        "\n",
        "    with gr.Row():\n",
        "        btn_video = gr.Button(\"送信（Submit）\")\n",
        "        btn_clear = gr.ClearButton(value=\"クリア（Clear）\", components=[input_kps, input_audio, out_video])\n",
        "\n",
        "    btn_vkps.click(ex_kps_audio, inputs=[input_video], outputs=[kps_audio_file, input_kps, input_audio])\n",
        "    btn_video.click(fix_face_video, inputs=[retarget_strategy, image, input_kps, input_audio], outputs=[out_video])\n",
        "\n",
        "\n",
        "demo.queue()\n",
        "demo.launch(share=True, debug=True)"
      ]
    }
  ]
}